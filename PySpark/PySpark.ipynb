{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1fba015",
   "metadata": {},
   "source": [
    "###                                                                                         Name: Sharon Laurance Muthipeedika \n",
    "###                                                                                         Matriculation No: 312486"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af7903",
   "metadata": {},
   "source": [
    "### Distributed Computing with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b05463",
   "metadata": {},
   "source": [
    "## Exercise 1: Apache Spark Basics\n",
    "### Part a) Basic Operations on Resilient Distributed Dataset (RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc2eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Import required Libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lag, when, isnull, sum, count, lit, avg, stddev,col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import findspark\n",
    "findspark.init()\n",
    "#Create Spark Session\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "sparkContext=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac83c64",
   "metadata": {},
   "source": [
    "### Create two RDD objects of a, b and do the following tasks. Words should be remained in the results of join operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "950e499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"spark\", \"rdd\", \"python\", \"context\", \"create\", \"class\"]\n",
    "b = [\"operation\", \"apache\", \"scala\", \"lambda\",\"parallel\",\"partition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43031ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_a=sparkContext.parallelize(a)\n",
    "rdd_b=sparkContext.parallelize(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bbfbbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_a.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806add47",
   "metadata": {},
   "source": [
    "### 1. Perform rightOuterJoin and fullOuterJoin operations between a and b. Briefly explain your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed4d7d",
   "metadata": {},
   "source": [
    "#### Right Outer Join selects the common words from both the lists and all the words from the list written on the right which is rdd_b. Therefore the result is 6 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3984e9c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('parallel', (None, 1)),\n",
       " ('lambda', (None, 1)),\n",
       " ('scala', (None, 1)),\n",
       " ('operation', (None, 1)),\n",
       " ('apache', (None, 1)),\n",
       " ('partition', (None, 1))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_a.map(lambda x:(x,1)).rightOuterJoin(rdd_b.map(lambda x:(x,1))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602b4247",
   "metadata": {},
   "source": [
    "#### FULL OUTER JOIN selects the common rows as well as all the remaining rows from both of the tables. Therefore there are 12 words. I tried doing only fullOuterJoin but it was giving letters instead of words. So, map is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c63225b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('python', (1, None)),\n",
       " ('spark', (1, None)),\n",
       " ('context', (1, None)),\n",
       " ('create', (1, None)),\n",
       " ('parallel', (None, 1)),\n",
       " ('lambda', (None, 1)),\n",
       " ('class', (1, None)),\n",
       " ('rdd', (1, None)),\n",
       " ('scala', (None, 1)),\n",
       " ('operation', (None, 1)),\n",
       " ('apache', (None, 1)),\n",
       " ('partition', (None, 1))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_a.map(lambda x:(x,1)).fullOuterJoin(rdd_b.map(lambda x:(x,1))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc17d7ec",
   "metadata": {},
   "source": [
    "### 2. Using map and reduce functions to count how many times the character \"s\" appears in all a and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bdf4446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of times s appears in a and b: 4\n"
     ]
    }
   ],
   "source": [
    "mapper = lambda x:x.count('s')\n",
    "print(f\"Number of times s appears in a and b: {rdd_a.map(mapper).reduce(lambda x, y: x + y)+rdd_b.map(mapper).reduce(lambda x, y: x + y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58077b01",
   "metadata": {},
   "source": [
    "### 3. Using aggregate function to count how many times the character \"s\" appears in all a and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d9e2106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of times s appears in a and b: 4\n"
     ]
    }
   ],
   "source": [
    "fn1 = lambda x,y : x + y.count('s')\n",
    "fn2 = lambda x,y : x+y\n",
    "print(f\"number of times s appears in a and b: {rdd_a.aggregate(0, fn1,fn2) + rdd_b.aggregate(0, fn1,fn2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f26d0",
   "metadata": {},
   "source": [
    "### Part b) Basic Operations on DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af1e7f",
   "metadata": {},
   "source": [
    "### Read the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef6e483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"D:/Data Analytics/DDA LAB/Lab 9/students.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d551077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|     null|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|  null|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|     null|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f41b9b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- course: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- points: long (nullable = true)\n",
      " |-- s_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9771e0",
   "metadata": {},
   "source": [
    "### 1. Replace the null value(s) in column points by the mean of all points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d0df98",
   "metadata": {},
   "source": [
    "#### Here the fillna() function is used. So first all columns with null are replaced by zero. Then the mean is found out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6db2496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|     null|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|     null|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "df_mean=df.fillna(value=0, subset=['points']).select(mean(df.columns[4]).alias('avg')).collect()\n",
    "avg=df_mean[0]['avg']\n",
    "df=df.fillna(value=avg,subset=[\"points\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d26d21",
   "metadata": {},
   "source": [
    "### 2. Replace the null value(s) in column dob and column last name by \"unknown\" and \"--\" respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a4c64de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|       --|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|           unknown|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|       --|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.fillna(value=\"unknown\",subset=[\"dob\"])\n",
    "df=df.fillna(value=\"--\",subset=[\"last_name\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde0049a",
   "metadata": {},
   "source": [
    "### 3. In the dob column, there exist several formats of dates, e.g. October 14, 1983 and 26 December 1989. Let’s convert all the dates into DD-MM-YYYY format where DD, MM and YYYY are two digits for day, two digits for months and four digits for year respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "338d72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f19a3",
   "metadata": {},
   "source": [
    "#### In the convert_date function formats are taken according to spark 3.x. To accomodate this LEGACY policy is set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1756e01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+----------+---------+------+----+\n",
      "|            course|   new_dob|first_name|last_name|points|s_id|\n",
      "+------------------+----------+----------+---------+------+----+\n",
      "|Humanities and Art|14-10-1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|26-09-1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|12-06-1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|05-04-1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|01-11-1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|17-02-1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|01-01-1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|13-01-1978|      John|       --|    10|   8|\n",
      "|  Machine Learning|26-12-1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|30-12-1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|12-06-1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|02-07-1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|22-07-1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|07-02-1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|18-05-1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|10-08-1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|16-12-1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|      null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|07-03-1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|02-06-1985|   Zachary|       --|    10|  20|\n",
      "+------------------+----------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date \n",
    "from pyspark.sql.functions import datediff,col\n",
    "# df1=df.select(to_date(df.dob, 'MMMM dd, yyyy').alias('dt'))\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "def convert_date(col, formats=(\"MMMM dd, yyyy\", \"dd MMMM yyyy\")):\n",
    "    return coalesce(*[to_date(col, f) for f in formats])\n",
    "\n",
    "df = df.select(\"course\", date_format(convert_date(df.dob),\"dd-MM-yyyy\").alias(\"new_dob\"),\"first_name\", \"last_name\", \"points\", \"s_id\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e8e1f",
   "metadata": {},
   "source": [
    "### 4. Insert a new column age and calculate the current age of all students. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f07b5",
   "metadata": {},
   "source": [
    "#### datediff() function is used to calculate the difference between current date and \"new_dob\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a6e2192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+------------------+----------+---------+------+----+\n",
      "|            course|   new_dob|               age|first_name|last_name|points|s_id|\n",
      "+------------------+----------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|14-10-1983| 38.74520547945205|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|26-09-1980|41.794520547945204|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|12-06-1982|40.084931506849315|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|05-04-1987| 35.26849315068493|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|01-11-1978|  43.6986301369863|      Kira| Schommer|    11|   5|\n",
      "|          Business|17-02-1981|              41.4| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|01-01-1984| 38.52876712328767|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|13-01-1978|  44.4986301369863|      John|       --|    10|   8|\n",
      "|  Machine Learning|26-12-1989| 32.53972602739726|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|30-12-1987| 34.53150684931507|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|12-06-1975| 47.09041095890411|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|02-07-1985| 37.02739726027397|     April|    Black|    11|  12|\n",
      "|  Computer Science|22-07-1980| 41.97534246575343|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|07-02-1986| 36.42465753424658|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|18-05-1987| 35.15068493150685|     Rosie|   Norman|     9|  15|\n",
      "|          Business|10-08-1984| 37.92054794520548|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|16-12-1990|31.567123287671233|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|      null|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|07-03-1980| 42.35068493150685|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|02-06-1985| 37.10958904109589|   Zachary|       --|    10|  20|\n",
      "+------------------+----------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_age(col):\n",
    "    return datediff(current_timestamp(),to_date(col,\"dd-MM-yyyy\"))/365\n",
    "\n",
    "df = df.select(\"course\", \"new_dob\", calculate_age(\"new_dob\").alias(\"age\"),\"first_name\", \"last_name\", \"points\", \"s_id\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d9735ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=df.agg({\"points\":\"stddev\"}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64810b2",
   "metadata": {},
   "source": [
    "### 5. Let’s consider granting some points for good performed students in the class. For each student, if his point is larger than 1 standard deviation of all points, then we update his current point to 20, which is the maximum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3efe36",
   "metadata": {},
   "source": [
    "#### Here one standard deviation is considered. So mean + stddev is done. The value is collected as val. It is compared with each value in the column. \n",
    " - If the current points are greater than val, then point is updated as 20. Otherwise its kept the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98aa4ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+------------------+----------+---------+------+----+\n",
      "|            course|   new_dob|               age|first_name|last_name|points|s_id|\n",
      "+------------------+----------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|14-10-1983| 38.74520547945205|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|26-09-1980|41.794520547945204|    Martin|  Genberg|    20|   2|\n",
      "|    Graphic Design|12-06-1982|40.084931506849315|     Athur|   Watson|    20|   3|\n",
      "|    Graphic Design|05-04-1987| 35.26849315068493|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|01-11-1978|  43.6986301369863|      Kira| Schommer|    11|   5|\n",
      "|          Business|17-02-1981|              41.4| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|01-01-1984| 38.52876712328767|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|13-01-1978|  44.4986301369863|      John|       --|    10|   8|\n",
      "|  Machine Learning|26-12-1989| 32.53972602739726|    Marcus|   Carson|    20|   9|\n",
      "|           Physics|30-12-1987| 34.53150684931507|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|12-06-1975| 47.09041095890411|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|02-07-1985| 37.02739726027397|     April|    Black|    11|  12|\n",
      "|  Computer Science|22-07-1980| 41.97534246575343|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|07-02-1986| 36.42465753424658|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|18-05-1987| 35.15068493150685|     Rosie|   Norman|     9|  15|\n",
      "|          Business|10-08-1984| 37.92054794520548|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|16-12-1990|31.567123287671233|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|      null|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|07-03-1980| 42.35068493150685|   Darlene|    Mills|    20|  19|\n",
      "|    Data Analytics|02-06-1985| 37.10958904109589|   Zachary|       --|    10|  20|\n",
      "+------------------+----------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stats = df.select((stddev(\"points\") + mean(\"points\")).alias('one_std_dev'))\n",
    "val = df_stats.collect()[0][0]\n",
    "val\n",
    "df = df.withColumn(\"points\", when(df.points > val, 20).otherwise(df.points))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcf4dd6",
   "metadata": {},
   "source": [
    "### Final Dataset Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e3b1e2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+------------------+----------+---------+------+----+\n",
      "|            course|   new_dob|               age|first_name|last_name|points|s_id|\n",
      "+------------------+----------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|14-10-1983| 38.74520547945205|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|26-09-1980|41.794520547945204|    Martin|  Genberg|    20|   2|\n",
      "|    Graphic Design|12-06-1982|40.084931506849315|     Athur|   Watson|    20|   3|\n",
      "|    Graphic Design|05-04-1987| 35.26849315068493|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|01-11-1978|  43.6986301369863|      Kira| Schommer|    11|   5|\n",
      "|          Business|17-02-1981|              41.4| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|01-01-1984| 38.52876712328767|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|13-01-1978|  44.4986301369863|      John|       --|    10|   8|\n",
      "|  Machine Learning|26-12-1989| 32.53972602739726|    Marcus|   Carson|    20|   9|\n",
      "|           Physics|30-12-1987| 34.53150684931507|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|12-06-1975| 47.09041095890411|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|02-07-1985| 37.02739726027397|     April|    Black|    11|  12|\n",
      "|  Computer Science|22-07-1980| 41.97534246575343|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|07-02-1986| 36.42465753424658|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|18-05-1987| 35.15068493150685|     Rosie|   Norman|     9|  15|\n",
      "|          Business|10-08-1984| 37.92054794520548|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|16-12-1990|31.567123287671233|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|   unknown|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|07-03-1980| 42.35068493150685|   Darlene|    Mills|    20|  19|\n",
      "|    Data Analytics|02-06-1985| 37.10958904109589|   Zachary|       --|    10|  20|\n",
      "+------------------+----------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.fillna(value=\"unknown\",subset=[\"new_dob\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f08f988a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|           points|\n",
      "+-------+-----------------+\n",
      "|  count|               20|\n",
      "|   mean|            12.35|\n",
      "| stddev|4.331949846626136|\n",
      "|    min|                6|\n",
      "|    max|               20|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"points\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39ef2509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark-dist-explore in c:\\users\\sharon\\anaconda3\\lib\\site-packages (0.1.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\sharon\\anaconda3\\lib\\site-packages (from pyspark-dist-explore) (1.2.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sharon\\anaconda3\\lib\\site-packages (from pyspark-dist-explore) (3.3.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\sharon\\anaconda3\\lib\\site-packages (from pyspark-dist-explore) (1.6.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\sharon\\anaconda3\\lib\\site-packages (from pyspark-dist-explore) (1.20.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\sharon\\anaconda3\\lib\\site-packages (from matplotlib->pyspark-dist-explore) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\sharon\\anaconda3\\lib\\site-packages (from matplotlib->pyspark-dist-explore) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sharon\\anaconda3\\lib\\site-packages (from matplotlib->pyspark-dist-explore) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sharon\\anaconda3\\lib\\site-packages (from matplotlib->pyspark-dist-explore) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sharon\\anaconda3\\lib\\site-packages (from matplotlib->pyspark-dist-explore) (8.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\sharon\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->pyspark-dist-explore) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\sharon\\anaconda3\\lib\\site-packages (from pandas->pyspark-dist-explore) (2021.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark-dist-explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8dd0dc",
   "metadata": {},
   "source": [
    "### 6. Create a histogram on the new points created in the task 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7035d1a0",
   "metadata": {},
   "source": [
    "#### Here histogram is created with 7 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68412517",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 2., 7., 4., 1., 0., 4.]),\n",
       " array([ 6,  8, 10, 12, 14, 16, 18, 20]),\n",
       " <BarContainer object of 7 artists>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark_dist_explore import hist\n",
    "import matplotlib.pyplot as plt\n",
    "# df.hist('points');\n",
    "fig, ax = plt.subplots()\n",
    "hist(ax, df.select('points'), bins = 7, color=['blue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8775ba",
   "metadata": {},
   "source": [
    "## Exercise 2: Manipulating Recommender Dataset with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f7acba",
   "metadata": {},
   "source": [
    "#### While reading the file header is not given. Therefore we use the alias to give “UserID::MovieID::Tag::Timestamp” to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfe02a1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n",
      "+------+-------+--------------------+----------+\n",
      "|UserID|MovieID|                 Tag| Timestamp|\n",
      "+------+-------+--------------------+----------+\n",
      "|    15|   4973|          excellent!|1215184630|\n",
      "|    20|   1747|            politics|1188263867|\n",
      "|    20|   1747|              satire|1188263867|\n",
      "|    20|   2424|     chick flick 212|1188263835|\n",
      "|    20|   2424|               hanks|1188263835|\n",
      "|    20|   2424|                ryan|1188263835|\n",
      "|    20|   2947|              action|1188263755|\n",
      "|    20|   2947|                bond|1188263756|\n",
      "|    20|   3033|               spoof|1188263880|\n",
      "|    20|   3033|           star wars|1188263880|\n",
      "|    20|   7438|              bloody|1188263801|\n",
      "|    20|   7438|             kung fu|1188263801|\n",
      "|    20|   7438|           Tarantino|1188263801|\n",
      "|    21|  55247|                   R|1205081506|\n",
      "|    21|  55253|               NC-17|1205081488|\n",
      "|    25|     50|        Kevin Spacey|1166101426|\n",
      "|    25|   6709|         Johnny Depp|1162147221|\n",
      "|    31|     65|        buddy comedy|1188263759|\n",
      "|    31|    546|strangely compelling|1188263674|\n",
      "|    31|   1091|         catastrophe|1188263741|\n",
      "+------+-------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create Spark Session\n",
    "spark = SparkSession.builder.appName('Movielens Data').getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "##Read the tags.dat file\n",
    "df = spark.read.option(\"delimiter\", \"::\").csv(\"D:/Data Analytics/DDA LAB/Lab 9/tags.dat\")\n",
    "df.printSchema()\n",
    "## Rename the column Names\n",
    "df = df.select(col(\"_c0\").alias(\"UserID\"), col(\"_c1\").alias(\"MovieID\"), col(\"_c2\").alias(\"Tag\"), col(\"_c3\").alias(\"Timestamp\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3c5c7",
   "metadata": {},
   "source": [
    "#### Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970. So, from_unixtime() function is used to convert to normal timestamp. This is a string which is then casted to timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbc20de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+-------------------+\n",
      "|UserID|MovieID|Tag                 |timestamp          |\n",
      "+------+-------+--------------------+-------------------+\n",
      "|15    |4973   |excellent!          |2008-07-04 17:17:10|\n",
      "|20    |1747   |politics            |2007-08-28 03:17:47|\n",
      "|20    |1747   |satire              |2007-08-28 03:17:47|\n",
      "|20    |2424   |chick flick 212     |2007-08-28 03:17:15|\n",
      "|20    |2424   |hanks               |2007-08-28 03:17:15|\n",
      "|20    |2424   |ryan                |2007-08-28 03:17:15|\n",
      "|20    |2947   |action              |2007-08-28 03:15:55|\n",
      "|20    |2947   |bond                |2007-08-28 03:15:56|\n",
      "|20    |3033   |spoof               |2007-08-28 03:18:00|\n",
      "|20    |3033   |star wars           |2007-08-28 03:18:00|\n",
      "|20    |7438   |bloody              |2007-08-28 03:16:41|\n",
      "|20    |7438   |kung fu             |2007-08-28 03:16:41|\n",
      "|20    |7438   |Tarantino           |2007-08-28 03:16:41|\n",
      "|21    |55247  |R                   |2008-03-09 17:51:46|\n",
      "|21    |55253  |NC-17               |2008-03-09 17:51:28|\n",
      "|25    |50     |Kevin Spacey        |2006-12-14 14:03:46|\n",
      "|25    |6709   |Johnny Depp         |2006-10-29 19:40:21|\n",
      "|31    |65     |buddy comedy        |2007-08-28 03:15:59|\n",
      "|31    |546    |strangely compelling|2007-08-28 03:14:34|\n",
      "|31    |1091   |catastrophe         |2007-08-28 03:15:41|\n",
      "+------+-------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert timestamp to unix timestamp\n",
    "df2 = df.withColumn(\"timestamp\" ,F.from_unixtime(col(\"Timestamp\")).cast(\"timestamp\"))\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0f454",
   "metadata": {},
   "source": [
    "### 1. A tagging session for a user can be defined as the duration in which he/she generated tagging activities. Typically, an inactive duration of 30 mins is considered as a termination of the tagging session. Your task is to separate out tagging sessions for each user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4fcfb",
   "metadata": {},
   "source": [
    "### To perform an operation on a group first, we need to partition the data using Window.partitionBy() , and for row number we need to additionally order by on partition data using orderBy clause. For lag(Column, offset) returns the value that is \"offset\" (i.e. 1) rows before the current row, and \"null\" if there is less than 1 row before the current row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b22cdd3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------------+-------------------+-------------------+-----------+----------------+\n",
      "|UserID|MovieID|             Tag|          timestamp|                lag|diff_in_sec|session_complete|\n",
      "+------+-------+----------------+-------------------+-------------------+-----------+----------------+\n",
      "|  1000|    277|children's story|2007-08-31 06:05:11|               null|       null|               0|\n",
      "|  1000|   1994|    sci-fi. dark|2007-08-31 06:05:36|2007-08-31 06:05:11|         25|               0|\n",
      "|  1000|   5377|         romance|2007-08-31 06:05:50|2007-08-31 06:05:36|         14|               0|\n",
      "|  1000|   7147|    family bonds|2007-08-31 06:06:01|2007-08-31 06:05:50|         11|               0|\n",
      "|  1000|    362|animated classic|2007-08-31 06:06:11|2007-08-31 06:06:01|         10|               0|\n",
      "|  1000|    276|          family|2007-08-31 06:07:15|2007-08-31 06:06:11|         64|               0|\n",
      "| 10003|  42013|        Passable|2006-06-16 06:33:55|               null|       null|               0|\n",
      "| 10003|  51662|  FIOS on demand|2008-04-12 00:35:26|2006-06-16 06:33:55|   57520891|               1|\n",
      "| 10003|  54997|  FIOS on demand|2008-04-12 00:35:35|2008-04-12 00:35:26|          9|               0|\n",
      "| 10003|  55765|  FIOS on demand|2008-04-12 00:35:42|2008-04-12 00:35:35|          7|               0|\n",
      "| 10003|  55363|  FIOS on demand|2008-04-12 00:37:00|2008-04-12 00:35:42|         78|               0|\n",
      "| 10003|  56152|  FIOS on demand|2008-04-12 00:38:46|2008-04-12 00:37:00|        106|               0|\n",
      "| 10003|  55116|  FIOS on demand|2008-04-12 00:40:36|2008-04-12 00:38:46|        110|               0|\n",
      "| 10003|  56174|  FIOS on demand|2008-04-12 00:41:10|2008-04-12 00:40:36|         34|               0|\n",
      "| 10003|  55176|  FIOS on demand|2008-04-12 00:42:35|2008-04-12 00:41:10|         85|               0|\n",
      "| 10003|  55247|  FIOS on demand|2008-04-12 00:42:36|2008-04-12 00:42:35|          1|               0|\n",
      "| 10003|  54881|  FIOS on demand|2008-04-12 00:42:38|2008-04-12 00:42:36|          2|               0|\n",
      "| 10003|  55820|  FIOS on demand|2008-04-12 00:44:33|2008-04-12 00:42:38|        115|               0|\n",
      "| 10003|  53123|  FIOS on demand|2008-04-12 00:44:35|2008-04-12 00:44:33|          2|               0|\n",
      "| 10003|  53550|  FIOS on demand|2008-04-12 00:45:37|2008-04-12 00:44:35|         62|               0|\n",
      "+------+-------+----------------+-------------------+-------------------+-----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window  = Window.partitionBy(\"UserID\").orderBy(\"timestamp\")\n",
    "\n",
    "## Calculating lag column here\n",
    "df3=df2.withColumn(\"lag\",lag(\"timestamp\",1).over(window))\n",
    "\n",
    "df4=df3.withColumn(\"diff_in_sec\",col(\"timestamp\").cast(\"long\")-col(\"lag\").cast(\"long\"))\n",
    "\n",
    "df5=df4.withColumn(\"session_complete\",when(col(\"diff_in_sec\")>30*60,1).otherwise(0))\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79299449",
   "metadata": {},
   "source": [
    "#### Here for a user a session stars with zero\n",
    "#### If the difference is greater than 30 min(i.e. 1800 sec) then other session is created. Otherwise the same session continues. \n",
    "#### For eg. for UserID 1000 diff_in_sec is always less than 1800 sec so same session continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c9af169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------------+-------------------+-------------------+-----------+----------------+----------+\n",
      "|UserID|MovieID|             Tag|          timestamp|                lag|diff_in_sec|session_complete|session_id|\n",
      "+------+-------+----------------+-------------------+-------------------+-----------+----------------+----------+\n",
      "|  1000|    277|children's story|2007-08-31 06:05:11|               null|       null|               0|         0|\n",
      "|  1000|   1994|    sci-fi. dark|2007-08-31 06:05:36|2007-08-31 06:05:11|         25|               0|         0|\n",
      "|  1000|   5377|         romance|2007-08-31 06:05:50|2007-08-31 06:05:36|         14|               0|         0|\n",
      "|  1000|   7147|    family bonds|2007-08-31 06:06:01|2007-08-31 06:05:50|         11|               0|         0|\n",
      "|  1000|    362|animated classic|2007-08-31 06:06:11|2007-08-31 06:06:01|         10|               0|         0|\n",
      "|  1000|    276|          family|2007-08-31 06:07:15|2007-08-31 06:06:11|         64|               0|         0|\n",
      "| 10003|  42013|        Passable|2006-06-16 06:33:55|               null|       null|               0|         0|\n",
      "| 10003|  51662|  FIOS on demand|2008-04-12 00:35:26|2006-06-16 06:33:55|   57520891|               1|         1|\n",
      "| 10003|  54997|  FIOS on demand|2008-04-12 00:35:35|2008-04-12 00:35:26|          9|               0|         1|\n",
      "| 10003|  55765|  FIOS on demand|2008-04-12 00:35:42|2008-04-12 00:35:35|          7|               0|         1|\n",
      "| 10003|  55363|  FIOS on demand|2008-04-12 00:37:00|2008-04-12 00:35:42|         78|               0|         1|\n",
      "| 10003|  56152|  FIOS on demand|2008-04-12 00:38:46|2008-04-12 00:37:00|        106|               0|         1|\n",
      "| 10003|  55116|  FIOS on demand|2008-04-12 00:40:36|2008-04-12 00:38:46|        110|               0|         1|\n",
      "| 10003|  56174|  FIOS on demand|2008-04-12 00:41:10|2008-04-12 00:40:36|         34|               0|         1|\n",
      "| 10003|  55176|  FIOS on demand|2008-04-12 00:42:35|2008-04-12 00:41:10|         85|               0|         1|\n",
      "| 10003|  55247|  FIOS on demand|2008-04-12 00:42:36|2008-04-12 00:42:35|          1|               0|         1|\n",
      "| 10003|  54881|  FIOS on demand|2008-04-12 00:42:38|2008-04-12 00:42:36|          2|               0|         1|\n",
      "| 10003|  55820|  FIOS on demand|2008-04-12 00:44:33|2008-04-12 00:42:38|        115|               0|         1|\n",
      "| 10003|  53123|  FIOS on demand|2008-04-12 00:44:35|2008-04-12 00:44:33|          2|               0|         1|\n",
      "| 10003|  53550|  FIOS on demand|2008-04-12 00:45:37|2008-04-12 00:44:35|         62|               0|         1|\n",
      "+------+-------+----------------+-------------------+-------------------+-----------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(df5.UserID).orderBy(df5.timestamp).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df6 = df5.withColumn('session_id', F.sum(col(\"session_complete\")).over(w))\n",
    "\n",
    "df6.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd097a9",
   "metadata": {},
   "source": [
    "### 2. Once you have all the tagging sessions for each user, calculate the frequency of tagging for each user session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1538736b",
   "metadata": {},
   "source": [
    "### - Here grouping is done by UserID and session Id, this gives unique number of sessions for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c8ca2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+\n",
      "|UserId|session_id|frequency|\n",
      "+------+----------+---------+\n",
      "|  1000|         0|        6|\n",
      "| 10003|         0|        1|\n",
      "| 10003|         1|       18|\n",
      "| 10003|         2|       38|\n",
      "| 10020|         0|        2|\n",
      "| 10025|         0|        1|\n",
      "| 10032|         0|       39|\n",
      "| 10032|         1|        1|\n",
      "| 10032|         2|        1|\n",
      "| 10032|         3|        1|\n",
      "| 10032|         4|        4|\n",
      "| 10032|         5|        1|\n",
      "| 10032|         6|        1|\n",
      "| 10032|         7|        4|\n",
      "| 10032|         8|        1|\n",
      "| 10032|         9|        1|\n",
      "| 10032|        10|        1|\n",
      "| 10032|        11|        1|\n",
      "| 10051|         0|        1|\n",
      "| 10058|         0|       35|\n",
      "+------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "tagging_df=df6.groupby(\"UserId\",\"session_id\").agg(F.count(\"session_id\").alias(\"frequency\"))\n",
    "tagging_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268b8173",
   "metadata": {},
   "source": [
    "### 3. Find a mean and standard deviation of the tagging frequency of each user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d81551",
   "metadata": {},
   "source": [
    "### - Here grouping by \"UserID\" is done and Mean and Standard Deviation of each user is found out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fbb031b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|UserId|              Mean|\n",
      "+------+------------------+\n",
      "|  1000|               6.0|\n",
      "| 10003|              19.0|\n",
      "| 10020|               2.0|\n",
      "| 10025|               1.0|\n",
      "| 10032| 4.666666666666667|\n",
      "| 10051|               1.0|\n",
      "| 10058|25.333333333333332|\n",
      "| 10059|               2.5|\n",
      "| 10064|               1.0|\n",
      "| 10084|              3.75|\n",
      "| 10094|               2.0|\n",
      "|  1010|               4.0|\n",
      "| 10117|               1.5|\n",
      "| 10125|              21.0|\n",
      "| 10132|            1.5625|\n",
      "| 10154|               8.0|\n",
      "| 10167|               1.0|\n",
      "|  1017|               7.0|\n",
      "| 10181|              11.0|\n",
      "| 10191|2.6666666666666665|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+------------------+\n",
      "|UserId|           Std_Dev|\n",
      "+------+------------------+\n",
      "|  1000|              null|\n",
      "| 10003|18.520259177452136|\n",
      "| 10020|              null|\n",
      "| 10025|              null|\n",
      "| 10032|10.873933246182093|\n",
      "| 10051|              null|\n",
      "| 10058|15.044378795195676|\n",
      "| 10059|0.7071067811865476|\n",
      "| 10064|              null|\n",
      "| 10084|2.0615528128088303|\n",
      "| 10094|              null|\n",
      "|  1010|              null|\n",
      "| 10117|0.7071067811865476|\n",
      "| 10125|              null|\n",
      "| 10132|1.4127396551853897|\n",
      "| 10154|              null|\n",
      "| 10167|              null|\n",
      "|  1017|              null|\n",
      "| 10181|              null|\n",
      "| 10191|0.5773502691896258|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagging_user_mean = tagging_df.groupBy(['UserId']).agg(F.mean('Frequency').alias('Mean'))\n",
    "tagging_user_std=tagging_df.groupBy(['UserId']).agg(F.stddev('Frequency').alias('Std_Dev'))\n",
    "tagging_user_mean.show()\n",
    "tagging_user_std.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8fdc88",
   "metadata": {},
   "source": [
    "### 4. Find a mean and standard deviation of the tagging frequency for across users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cb6c39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation across all users is 22.264293050265017\n",
      "Mean across all users is 7.300084014358817\n"
     ]
    }
   ],
   "source": [
    "std_across_users=tagging_df.agg({'frequency':'stddev'})\n",
    "mean_across_users=tagging_df.agg({'frequency':'mean'})\n",
    "\n",
    "std=std_across_users.collect()[0][0]\n",
    "mean=mean_across_users.collect()[0][0]\n",
    "\n",
    "print(\"Standard Deviation across all users is\",std)\n",
    "print(\"Mean across all users is\",mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb277d7",
   "metadata": {},
   "source": [
    "### 5. Provide the list of users with a mean tagging frequency within the two standard deviation from the mean frequency of all users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134309b5",
   "metadata": {},
   "source": [
    "### Here mean tagging frequency dataframe created in Question 3 is used. For each user is the current mean is less than two standard deviations from mean of all users which is calculated in Question 4. Rows are filtered based on this condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82433c83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|UserId|              Mean|\n",
      "+------+------------------+\n",
      "|  1000|               6.0|\n",
      "| 10003|              19.0|\n",
      "| 10020|               2.0|\n",
      "| 10025|               1.0|\n",
      "| 10032| 4.666666666666667|\n",
      "| 10051|               1.0|\n",
      "| 10058|25.333333333333332|\n",
      "| 10059|               2.5|\n",
      "| 10064|               1.0|\n",
      "| 10084|              3.75|\n",
      "| 10094|               2.0|\n",
      "|  1010|               4.0|\n",
      "| 10117|               1.5|\n",
      "| 10125|              21.0|\n",
      "| 10132|            1.5625|\n",
      "| 10154|               8.0|\n",
      "| 10167|               1.0|\n",
      "|  1017|               7.0|\n",
      "| 10181|              11.0|\n",
      "| 10191|2.6666666666666665|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_5=tagging_user_mean.filter(tagging_user_mean.Mean < 2 * std + mean)\n",
    "df_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c4b03f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3962"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde7346c",
   "metadata": {},
   "source": [
    "#### References:\n",
    " - https://netflixsub.com/how-to-replace-null-values-with-mean-in-pyspark-dataframe/#:~:text=Alternatively%2C%20if%20there%20are%20null,null%20with%20such%20mean%20taken.\n",
    " - https://copypaste.guru/WhereIsMyPythonModule/how-to-fix-modulenotfounderror-no-module-named-pyspark-dist-explore\n",
    " - https://stackoverflow.com/questions/39154325/pyspark-show-histogram-of-a-data-frame-column\n",
    " - https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions/\n",
    " - https://sparkbyexamples.com/pyspark/pyspark-timestamp-difference-seconds-minutes-hours/\n",
    " - https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.Window.rowsBetween.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
